# DynRec Optimization Log
# Timestamp: 2025-04-08 15:50:20

[Configuration]
Loss Function: AmplitudePhaseL2
Loss Config: {"numeric":{"amp_weight":1.0,"phase_weight":1.0},"type":"AmplitudePhaseL2"}
Regularization: L2
Regularization Config: {"type":"L2"}
Lambda: 0.01
Target: 
(1.2,0.3) 
(2.1,0.4) 
(1.8,-0.2) 
...
Initial x0: 
0 
0 
0 


[Optimization]
Optimization Name: nag
Optimization Config: {"learning_rate":0.01,"max_iter":1000,"tolerance":1e-06}

[Iterations]
Iter	Loss	GradientNorm
0	9.48768690	0.14375932
1	9.39337768	9.50458174
2	7.72836932	8.64095694
3	5.77883441	7.46067858
4	3.86901341	6.08658488
5	2.25172184	4.61369771
6	1.07128353	3.12825359
7	0.36508094	1.70523048
8	0.08404446	0.41378714
9	0.12331061	0.74334935
10	0.35482819	1.67426848
11	0.65554444	2.39249442
12	0.92729703	2.89127679
13	1.10705675	3.17754133
14	1.16825335	3.26781418
15	1.11531606	3.18585860
16	0.97420464	2.96043737
17	0.78168326	2.62318000
18	0.57557988	2.20662506
19	0.38750044	1.74250443
20	0.23863688	1.26032197
21	0.13858572	0.78627969
22	0.08657946	0.34282161
23	0.07426212	0.06437382
24	0.08909755	0.39416756
25	0.11762995	0.66310180
26	0.14805149	0.86132976
27	0.17180169	0.98915745
28	0.18416972	1.05044269
29	0.18405697	1.05176011
30	0.17316502	1.00166823
31	0.15490639	0.91000200
32	0.13330568	0.78720933
33	0.11208907	0.64375502
34	0.09407390	0.48961210
35	0.08088662	0.33385852
36	0.07297139	0.18442404
37	0.06980987	0.04882668
38	0.07025618	0.07412910
39	0.07289636	0.17224092
40	0.07636132	0.24832820
41	0.07954966	0.30145049
42	0.08174461	0.33226857
43	0.08263215	0.34247428
44	0.08224361	0.33453054
45	0.08085276	0.31143447
46	0.07885803	0.27649293
47	0.07667470	0.23311700
48	0.07465348	0.18464261
49	0.07303295	0.13418504
50	0.07192522	0.08454418
51	0.07132835	0.03829744
52	0.07115620	0.00922906
53	0.07127557	0.04081009
54	0.07154196	0.06899808
55	0.07182803	0.08996382
56	0.07204143	0.10358919
57	0.07213168	0.11023590
58	0.07208775	0.11057305
59	0.07192933	0.10548679
60	0.07169478	0.09600246
61	0.07142914	0.08321339
62	0.07117393	0.06821754
63	0.07096046	0.05206346
64	0.07080662	0.03570747
65	0.07071697	0.01998662
66	0.07068514	0.00568635
67	0.07069763	0.00731342
68	0.07073777	0.01768512
69	0.07078932	0.02576391
70	0.07083901	0.03143482
71	0.07087786	0.03476177
72	0.07090152	0.03591846
73	0.07090961	0.03516062
74	0.07090465	0.03280103
75	0.07089076	0.02918597
76	0.07087242	0.02467373
77	0.07085358	0.01961597
78	0.07083704	0.01434336
79	0.07082435	0.00915939
80	0.07081582	0.00437563
81	0.07081085	0.00146347
82	0.07080834	0.00427938
83	0.07080703	0.00719379
84	0.07080582	0.00939698
85	0.07080395	0.01084215
86	0.07080107	0.01155863
87	0.07079723	0.01161307
88	0.07079276	0.01109661
89	0.07078815	0.01011583
90	0.07078392	0.00878499
91	0.07078052	0.00721928
92	0.07077824	0.00552914
93	0.07077721	0.00381607
94	0.07077739	0.00217151
95	0.07077858	0.00070547
96	0.07078054	0.00077916
97	0.07078296	0.00184301
98	0.07078555	0.00269243
99	0.07078807	0.00329357
100	0.07079032	0.00364991
101	0.07079218	0.00377854
102	0.07079361	0.00370572
103	0.07079460	0.00346399
104	0.07079517	0.00308954
105	0.07079538	0.00262004
106	0.07079528	0.00209264
107	0.07079492	0.00154268
108	0.07079437	0.00100373
109	0.07079366	0.00051519
110	0.07079284	0.00023272
111	0.07079193	0.00046364
112	0.07079097	0.00075686
113	0.07078998	0.00098538
114	0.07078899	0.00113713
115	0.07078804	0.00121339
116	0.07078714	0.00122049
117	0.07078633	0.00116765
118	0.07078561	0.00106590
119	0.07078501	0.00092715
120	0.07078452	0.00076344
121	0.07078416	0.00058642
122	0.07078391	0.00040684
123	0.07078377	0.00023470
124	0.07078371	0.00008399
125	0.07078374	0.00008536
126	0.07078382	0.00019411
127	0.07078395	0.00028343
128	0.07078411	0.00034723
129	0.07078428	0.00038550
130	0.07078445	0.00039990
131	0.07078462	0.00039312
132	0.07078478	0.00036855
133	0.07078492	0.00033001
134	0.07078504	0.00028146
135	0.07078514	0.00022690
136	0.07078522	0.00017022
137	0.07078529	0.00011540
138	0.07078534	0.00006798
139	0.07078537	0.00004281
140	0.07078540	0.00005687
141	0.07078541	0.00008309
142	0.07078543	0.00010540
143	0.07078544	0.00012062
144	0.07078545	0.00012833
145	0.07078547	0.00012898
146	0.07078548	0.00012344
147	0.07078551	0.00011281
148	0.07078553	0.00009833
149	0.07078557	0.00008126
150	0.07078560	0.00006283
151	0.07078564	0.00004424
152	0.07078568	0.00002678
153	0.07078573	0.00001299
154	0.07078577	0.00001255
155	0.07078581	0.00002200
156	0.07078585	0.00003084
157	0.07078588	0.00003734
158	0.07078591	0.00004129
159	0.07078594	0.00004279
160	0.07078596	0.00004208
161	0.07078598	0.00003952
162	0.07078599	0.00003549
163	0.07078599	0.00003042
164	0.07078599	0.00002472
165	0.07078599	0.00001881
166	0.07078598	0.00001314
167	0.07078597	0.00000832
168	0.07078596	0.00000571
169	0.07078594	0.00000661
170	0.07078592	0.00000904
171	0.07078591	0.00001127
172	0.07078589	0.00001284
173	0.07078588	0.00001367
174	0.07078586	0.00001379
175	0.07078585	0.00001327
176	0.07078584	0.00001223
177	0.07078583	0.00001081
178	0.07078582	0.00000914
179	0.07078581	0.00000738
180	0.07078581	0.00000570
181	0.07078580	0.00000429
182	0.07078580	0.00000343
183	0.07078580	0.00000330
184	0.07078580	0.00000370
185	0.07078580	0.00000423
186	0.07078580	0.00000468
187	0.07078580	0.00000494
188	0.07078580	0.00000499
189	0.07078580	0.00000484
190	0.07078581	0.00000451
191	0.07078581	0.00000403
192	0.07078581	0.00000346
193	0.07078581	0.00000282
194	0.07078581	0.00000215
195	0.07078581	0.00000151
196	0.07078581	0.00000094

[Result]
Final Loss: 0.07078581
Iterations: 196
Stop Reason: completed
x_opt: 1.09561562 1.10580078 1.19039254 

[Summary(JSON)]
{
    "final_loss": 0.07078580500333645,
    "iterations": 196,
    "stop_reason": "completed",
    "x_opt": [
        1.0956156245714757,
        1.1058007752760697,
        1.1903925370327777
    ]
}
