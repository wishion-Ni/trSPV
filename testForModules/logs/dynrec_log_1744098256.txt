# DynRec Optimization Log
# Timestamp: 2025-04-08 15:44:16

[Configuration]
Loss Function: AmplitudePhaseL2
Loss Config: {"numeric":{"amp_weight":1.0,"phase_weight":1.0},"type":"AmplitudePhaseL2"}
Regularization: L2
Regularization Config: {"type":"L2"}
Lambda: 0.01
Target: 
(1.2,0.3) 
(2.1,0.4) 
(1.8,-0.2) 
...
Initial x0: 
0 
0 
0 


[Optimization]
Optimization Name: momentum
Optimization Config: {"learning_rate":0.01,"max_iter":1000,"tolerance":1e-06}

[Iterations]
Iter	Loss	GradientNorm
0	9.48768690	0.14375932
1	9.40545959	9.51077425
2	8.49497010	9.06311093
3	6.96488633	8.19877486
4	5.12740398	7.02121628
5	3.31079910	5.61929308
6	1.78383138	4.08393257
7	0.71719203	2.50374591
8	0.17031164	0.96296903
9	0.10108732	0.49421543
10	0.39238593	1.75440404
11	0.88728315	2.81045307
12	1.42492890	3.62467611
13	1.87035680	4.18102485
14	2.13393274	4.47802968
15	2.17885081	4.52709863
16	2.01757353	4.35061597
17	1.69996509	3.97973758
18	1.29686331	3.45199028
19	0.88295882	2.80880570
20	0.52224183	2.09311745
21	0.25818772	1.34715326
22	0.10957363	0.61072368
23	0.07161900	0.09263110
24	0.12122758	0.70438947
25	0.22459153	1.22723123
26	0.34531707	1.63823223
27	0.45148706	1.92836543
28	0.52057124	2.09556038
29	0.54169549	2.14396968
30	0.51535570	2.08311761
31	0.45111282	1.92689260
32	0.36407325	1.69243618
33	0.27102882	1.39898828
34	0.18702680	1.06674725
35	0.12291500	0.71580470
36	0.08412473	0.36525709
37	0.07067856	0.03669966
38	0.07818813	0.27109094
39	0.09947127	0.52907564
40	0.12637509	0.73569048
41	0.15143214	0.88583959
42	0.16907867	0.97782057
43	0.17629510	1.01295707
44	0.17266132	0.99521732
45	0.15992758	0.93075284
46	0.14127146	0.82737947
47	0.12043614	0.69403029
48	0.10093002	0.54021065
49	0.08542222	0.37548602
50	0.07540718	0.20905563
51	0.07114845	0.05019638
52	0.07185883	0.09874214
53	0.07603897	0.22533077
54	0.08188232	0.32860330
55	0.08765979	0.40560281
56	0.09201749	0.45516641
57	0.09414971	0.47761066
58	0.09383908	0.47454850
59	0.09138182	0.44867677
60	0.08743347	0.40353514
61	0.08281849	0.34324895
62	0.07834532	0.27226968
63	0.07465971	0.19512577
64	0.07215571	0.11619643
65	0.07094990	0.03953851
66	0.07091124	0.03157755
67	0.07173076	0.09384332
68	0.07301060	0.14535215
69	0.07435261	0.18463206
70	0.07543021	0.21098800
71	0.07603368	0.22442715
72	0.07608561	0.22558622
73	0.07562927	0.21563703
74	0.07479734	0.19617589
75	0.07377014	0.16910340
76	0.07273334	0.13650185
77	0.07184265	0.10051668
78	0.07120081	0.06325082
79	0.07084862	0.02671023
80	0.07076908	0.00831273
81	0.07090125	0.03831912
82	0.07115960	0.06382495
83	0.07145413	0.08369787
84	0.07170747	0.09751289
85	0.07186649	0.10519777
86	0.07190720	0.10698925
87	0.07183345	0.10338854
88	0.07167072	0.09510993
89	0.07145721	0.08302442
90	0.07123426	0.06810161
91	0.07103804	0.05135255
92	0.07089376	0.03377657
93	0.07081303	0.01631528
94	0.07079416	0.00056830
95	0.07082492	0.01506940
96	0.07088668	0.02769890
97	0.07095895	0.03771848
98	0.07102341	0.04489891
99	0.07106684	0.04917378
100	0.07108247	0.05062522
101	0.07107005	0.04946481
102	0.07103457	0.04601034
103	0.07098441	0.04065968
104	0.07092913	0.03386342
105	0.07087753	0.02609782
106	0.07083618	0.01783949
107	0.07080867	0.00954511
108	0.07079551	0.00171067
109	0.07079465	0.00567211
110	0.07080234	0.01187720
111	0.07081414	0.01689556
112	0.07082589	0.02059144
113	0.07083436	0.02291533
114	0.07083771	0.02389064
115	0.07083552	0.02360448
116	0.07082861	0.02219683
117	0.07081862	0.01984838
118	0.07080755	0.01676765
119	0.07079730	0.01317800
120	0.07078933	0.00930535
121	0.07078447	0.00536707
122	0.07078286	0.00156509
123	0.07078404	0.00194865
124	0.07078717	0.00500245
125	0.07079118	0.00750940
126	0.07079508	0.00940010
127	0.07079805	0.01064381
128	0.07079962	0.01124456
129	0.07079963	0.01123735
130	0.07079824	0.01068332
131	0.07079582	0.00966418
132	0.07079286	0.00827613
133	0.07078986	0.00662377
134	0.07078722	0.00481420
135	0.07078521	0.00295180
136	0.07078396	0.00113568
137	0.07078344	0.00058082
138	0.07078350	0.00206658
139	0.07078394	0.00331146
140	0.07078456	0.00427137
141	0.07078517	0.00492741
142	0.07078563	0.00527786
143	0.07078590	0.00533612
144	0.07078597	0.00512855
145	0.07078591	0.00469185
146	0.07078578	0.00407021
147	0.07078567	0.00331242
148	0.07078565	0.00246906
149	0.07078576	0.00158988
150	0.07078601	0.00072180
151	0.07078637	0.00010789
152	0.07078680	0.00082896
153	0.07078725	0.00144568
154	0.07078765	0.00193063
155	0.07078795	0.00227311
156	0.07078814	0.00247062
157	0.07078818	0.00252802
158	0.07078810	0.00245651
159	0.07078791	0.00227246
160	0.07078765	0.00199609
161	0.07078734	0.00165012
162	0.07078702	0.00125837
163	0.07078672	0.00084455
164	0.07078645	0.00043116
165	0.07078622	0.00004318
166	0.07078603	0.00031912
167	0.07078589	0.00062340
168	0.07078578	0.00086709
169	0.07078570	0.00104420
170	0.07078563	0.00115273
171	0.07078558	0.00119424
172	0.07078555	0.00117340
173	0.07078553	0.00109746
174	0.07078552	0.00097564
175	0.07078553	0.00081846
176	0.07078555	0.00063713
177	0.07078559	0.00044294
178	0.07078564	0.00024672
179	0.07078570	0.00005945
180	0.07078577	0.00011626
181	0.07078583	0.00026552
182	0.07078588	0.00038730
183	0.07078593	0.00047812
184	0.07078596	0.00053658
185	0.07078598	0.00056307
186	0.07078598	0.00055949
187	0.07078597	0.00052900
188	0.07078595	0.00047579
189	0.07078592	0.00040472
190	0.07078589	0.00032104
191	0.07078586	0.00023009
192	0.07078583	0.00013702
193	0.07078580	0.00004661
194	0.07078577	0.00003714
195	0.07078575	0.00011060
196	0.07078574	0.00017134
197	0.07078573	0.00021766
198	0.07078572	0.00024875
199	0.07078572	0.00026460
200	0.07078572	0.00026598
201	0.07078573	0.00025426
202	0.07078574	0.00023132
203	0.07078575	0.00019941
204	0.07078577	0.00016098
205	0.07078579	0.00011856
206	0.07078580	0.00007462
207	0.07078582	0.00003155
208	0.07078583	0.00000981
209	0.07078585	0.00004515
210	0.07078586	0.00007522
211	0.07078586	0.00009866
212	0.07078587	0.00011495
213	0.07078587	0.00012402
214	0.07078587	0.00012614
215	0.07078586	0.00012190
216	0.07078586	0.00011214
217	0.07078585	0.00009789
218	0.07078584	0.00008030
219	0.07078583	0.00006056
220	0.07078582	0.00003984
221	0.07078581	0.00001925
222	0.07078580	0.00000059

[Result]
Final Loss: 0.07078580
Iterations: 222
Stop Reason: completed
x_opt: 1.09561612 1.10579961 1.19039272 

[Summary(JSON)]
{
    "final_loss": 0.07078579750739084,
    "iterations": 222,
    "stop_reason": "completed",
    "x_opt": [
        1.0956161168254857,
        1.1057996075116054,
        1.1903927172133022
    ]
}
