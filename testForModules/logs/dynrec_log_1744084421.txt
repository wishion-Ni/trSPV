# DynRec Optimization Log
# Timestamp: 2025-04-08 11:53:41

[Configuration]
Loss Function: AmplitudePhaseL2
Loss Config: {"numeric":{"amp_weight":1.0,"phase_weight":1.0},"type":"AmplitudePhaseL2"}
Regularization: L2
Regularization Config: {"type":"L2"}
Lambda: 0.01
Target: 
(1.2,0.3) 
(2.1,0.4) 
(1.8,-0.2) 
...
Initial x0: 
0 
0 
0 


[Iterations]
Iter	Loss	GradientNorm
1	9.48768690	0.14375932
2	8.59709890	9.11724652
3	8.01201881	8.79878527
4	7.36788549	8.43443945
5	6.71775026	8.05008344
6	6.08132083	7.65525296
7	5.46830037	7.25475951
8	4.88421515	6.85154252
9	4.33246623	6.44766171
10	3.81520631	6.04471797
11	3.33376493	5.64405679
12	2.88887385	5.24687584
13	2.48079491	4.85428555
14	2.10939693	4.46734430
15	1.77420496	4.08707914
16	1.47443441	3.71449720
17	1.20901733	3.35059127
18	0.97662525	2.99634105
19	0.77569145	2.65271149
20	0.60443445	2.32064905
21	0.46088407	2.00107710
22	0.34291038	1.69489188
23	0.24825617	1.40296224
24	0.17457238	1.12614052
25	0.11945627	0.86530616
26	0.08049122	0.62152051
27	0.05528733	0.39668186
28	0.04152119	0.19795016
29	0.03697383	0.10011871
30	0.03956514	0.22247473
31	0.04738367	0.37266301
32	0.05871085	0.51122184
33	0.07203858	0.63415335
34	0.08608014	0.74074723
35	0.09977405	0.83105323
36	0.11228124	0.90542737
37	0.12297626	0.96440213
38	0.13143304	1.00863457
39	0.13740645	1.03887851
40	0.14081057	1.05596549
41	0.14169491	1.06078949
42	0.14021946	1.05429334
43	0.13662973	1.03745639
44	0.13123260	1.01128292
45	0.12437362	0.97679129
46	0.11641640	0.93500384
47	0.10772459	0.88693741
48	0.09864651	0.83359464
49	0.08950285	0.77595589
50	0.08057719	0.71497189
51	0.07210951	0.65155710
52	0.06429235	0.58658379
53	0.05726940	0.52087681
54	0.05113634	0.45520929
55	0.04594346	0.39029929
56	0.04169984	0.32680777
57	0.03837853	0.26533892
58	0.03592261	0.20644550
59	0.03425161	0.15064896
60	0.03326801	0.09852033
61	0.03286362	0.05117870
62	0.03292543	0.01867456
63	0.03334097	0.04254109
64	0.03400277	0.07721554
65	0.03481210	0.10884522
66	0.03568173	0.13643485
67	0.03653782	0.15981450
68	0.03732097	0.17899840
69	0.03798644	0.19408428
70	0.03850375	0.20522435
71	0.03885569	0.21261247
72	0.03903687	0.21647598
73	0.03905196	0.21706929
74	0.03891377	0.21466824
75	0.03864125	0.20956482
76	0.03825749	0.20206227
77	0.03778790	0.19247038
78	0.03725852	0.18110111
79	0.03669461	0.16826449
80	0.03611948	0.15426493
81	0.03555365	0.13939780
82	0.03501424	0.12394651
83	0.03451466	0.10818011
84	0.03406456	0.09235147
85	0.03366990	0.07669657
86	0.03333331	0.06143566
87	0.03305449	0.04677939
88	0.03283071	0.03295121
89	0.03265736	0.02028822
90	0.03252853	0.00997425
91	0.03243752	0.00878259
92	0.03237733	0.01646377
93	0.03234111	0.02470955
94	0.03232244	0.03210965
95	0.03231564	0.03842276
96	0.03231590	0.04359219
97	0.03231940	0.04761947
98	0.03232332	0.05053531
99	0.03232577	0.05238955
100	0.03232572	0.05324615
101	0.03232289	0.05317997
102	0.03231755	0.05227427
103	0.03231041	0.05061850
104	0.03230245	0.04830629
105	0.03229475	0.04543366
106	0.03228838	0.04209731
107	0.03228433	0.03839313
108	0.03228336	0.03441479
109	0.03228603	0.03025264
110	0.03229260	0.02599278
111	0.03230308	0.02171651
112	0.03231723	0.01750053
113	0.03233459	0.01341869
114	0.03235453	0.00954925
115	0.03237629	0.00600615
116	0.03239905	0.00314187
117	0.03242197	0.00263921
118	0.03244424	0.00463043
119	0.03246512	0.00689151
120	0.03248396	0.00893100
121	0.03250027	0.01065786
122	0.03251365	0.01204900
123	0.03252388	0.01310268
124	0.03253086	0.01382764
125	0.03253462	0.01423944
126	0.03253531	0.01435854
127	0.03253316	0.01420916
128	0.03252848	0.01381832
129	0.03252163	0.01321505
130	0.03251300	0.01242967
131	0.03250298	0.01149311
132	0.03249196	0.01043633
133	0.03248030	0.00928978
134	0.03246834	0.00808297
135	0.03245638	0.00684407
136	0.03244467	0.00559971
137	0.03243342	0.00437513
138	0.03242279	0.00319510
139	0.03241290	0.00208837
140	0.03240386	0.00111497
141	0.03239569	0.00062302
142	0.03238845	0.00112739
143	0.03238213	0.00183112
144	0.03237671	0.00247412
145	0.03237218	0.00301981
146	0.03236850	0.00345954
147	0.03236563	0.00379216
148	0.03236352	0.00402010
149	0.03236211	0.00414815
150	0.03236136	0.00418280
151	0.03236120	0.00413192
152	0.03236158	0.00400435
153	0.03236245	0.00380969
154	0.03236373	0.00355800
155	0.03236537	0.00325955
156	0.03236730	0.00292462
157	0.03236946	0.00256331
158	0.03237178	0.00218534
159	0.03237421	0.00179990
160	0.03237667	0.00141560
161	0.03237913	0.00104041
162	0.03238151	0.00068198
163	0.03238377	0.00034995
164	0.03238588	0.00010671
165	0.03238779	0.00028361
166	0.03238948	0.00052460
167	0.03239092	0.00073681
168	0.03239210	0.00091431
169	0.03239302	0.00105582
170	0.03239367	0.00116153
171	0.03239407	0.00123246
172	0.03239421	0.00127036
173	0.03239413	0.00127752
174	0.03239383	0.00125666
175	0.03239335	0.00121086
176	0.03239272	0.00114345
177	0.03239195	0.00105789
178	0.03239108	0.00095774
179	0.03239013	0.00084652
180	0.03238914	0.00072769
181	0.03238812	0.00060453
182	0.03238711	0.00048015
183	0.03238611	0.00035743
184	0.03238516	0.00023902
185	0.03238427	0.00012767
186	0.03238345	0.00003282
187	0.03238271	0.00007822
188	0.03238206	0.00015981
189	0.03238150	0.00023129
190	0.03238104	0.00029092
191	0.03238068	0.00033831
192	0.03238041	0.00037351
193	0.03238024	0.00039685
194	0.03238016	0.00040893
195	0.03238015	0.00041054
196	0.03238023	0.00040263
197	0.03238036	0.00038628
198	0.03238056	0.00036267
199	0.03238080	0.00033303
200	0.03238107	0.00029860
201	0.03238138	0.00026065
202	0.03238170	0.00022038
203	0.03238204	0.00017898
204	0.03238237	0.00013754
205	0.03238270	0.00009713
206	0.03238302	0.00005894
207	0.03238331	0.00002563
208	0.03238359	0.00002060
209	0.03238383	0.00004595
210	0.03238405	0.00007089
211	0.03238423	0.00009223
212	0.03238438	0.00010948
213	0.03238450	0.00012251
214	0.03238459	0.00013139
215	0.03238464	0.00013628
216	0.03238466	0.00013743
217	0.03238466	0.00013517
218	0.03238463	0.00012987
219	0.03238458	0.00012196
220	0.03238451	0.00011187
221	0.03238443	0.00010006
222	0.03238434	0.00008698
223	0.03238425	0.00007307
224	0.03238415	0.00005877
225	0.03238404	0.00004449
226	0.03238394	0.00003064
227	0.03238385	0.00001774
228	0.03238376	0.00000760
229	0.03238368	0.00000981
230	0.03238360	0.00001862
231	0.03238354	0.00002689
232	0.03238349	0.00003386
233	0.03238345	0.00003936
234	0.03238343	0.00004336
235	0.03238341	0.00004589
236	0.03238340	0.00004702
237	0.03238341	0.00004685
238	0.03238342	0.00004551
239	0.03238344	0.00004314
240	0.03238346	0.00003990
241	0.03238350	0.00003596
242	0.03238353	0.00003149
243	0.03238357	0.00002664
244	0.03238361	0.00002158
245	0.03238365	0.00001646
246	0.03238369	0.00001142
247	0.03238373	0.00000660
248	0.03238376	0.00000220
249	0.03238379	0.00000235
250	0.03238382	0.00000592
251	0.03238385	0.00000906
252	0.03238387	0.00001167
253	0.03238388	0.00001373
254	0.03238389	0.00001522
255	0.03238390	0.00001616
256	0.03238391	0.00001658
257	0.03238391	0.00001652
258	0.03238390	0.00001603
259	0.03238390	0.00001515
260	0.03238389	0.00001396
261	0.03238388	0.00001251
262	0.03238386	0.00001087
263	0.03238385	0.00000909
264	0.03238384	0.00000725
265	0.03238382	0.00000539
266	0.03238381	0.00000358
267	0.03238380	0.00000187
268	0.03238379	0.00000055

[Result]
Final Loss: 0.03238379
Iterations: 1000
Stop Reason: completed
x_opt: 1.09561568 1.10580049 1.19039229 

[Summary(JSON)]
{
    "final_loss": 0.03238378523902464,
    "iterations": 1000,
    "stop_reason": "completed",
    "x_opt": [
        1.0956156817770109,
        1.1058004887757535,
        1.1903922937665652
    ]
}
